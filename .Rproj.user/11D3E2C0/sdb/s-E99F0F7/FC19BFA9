{
    "contents" : "# Designed and developed by Jacob Bieker (jacob@bieker.tech)\n\n# Script reads in a csv file containing spiked read counts from TCR sequencing, and calculates the \n# what number is needed to change the counts of each spiked read to the mean. \n# It then takes the multiples for the largest read and smallest read and uses that as the scale\n# to normalize the FASTQ files with all the reads. \n# Prior to the full FASTQ file being normalized, the spiked reads are removed.\n#\n# Assumptions:\n#   1.  A CSV file, named \"<FASTQ File>xout.csv\" per FASTQ file of the format ID,spike,count\n#   2.  A FASTQ file per CSV file\n#   3.  Spiked reads are supposed to be present in the exact same frequency\n\n#############################################################################\n#\n#             Setup\n#\n#############################################################################\n\n#############################################################################\n#\n#             Normalization functions\n#\n#############################################################################\n\n#############################################################################\n#\n#             Spiked Read CSV code\n#\n#############################################################################\n\n#   identify all .csv files that should be the spiked read counts in the directory \nfiles <- list.files(getwd(), pattern = \"*xout.csv\");\n\n# Go through each file and read in the CSV data, skpping the first line which gives no information\n# All operations on the data will happen inside the for loop, so that it goes through each file\n# and each FASTQ file once\nfor(spike_file in files) {\n  data <- read.csv(spike_file, header = FALSE, skip = 1);\n  #Get the mean from the last column, which is the read count\n  spiked_mean <- mean(data[[3]])\n  # The get max\n  spiked_max <- max(data[[3]])\n  # Get the min\n  spiked_min <- min(data[[3]])\n  # Get the smallest multiple that the FASTQ file will be normalized by\n  smallest_multiple <- (spiked_mean/spiked_max)\n  # Get the largest multiple that the FASTQ file will be normalized by\n  largest_multiple <- (spiked_mean/spiked_min)\n  # Getting the range for possible use at the scale\n  multiple_range <- largest_multiple - smallest_multiple\n  #Range of the spiked reads\n  spiked_range <- spiked_max - spiked_min\n  \n  # Test vector holding all the multiples needed to hit the mean\n  multiples_needed <- spiked_mean/data$V3 \n  \n  #Get the percentage of the range for each value, for later use with the multiple needed\n  percentages_raw <- vectorized_spikes - vectorized_small\n  percentage_change <- 100.00/max(percentages_raw)\n  percentages <- percentages_raw * percentage_change\n  \n  #Puts the data in the data.frame for later use\n  data$V4 <- multiples_needed\n  data$V5 <- percentages\n  \n  # New IDEA: Use the 260 spiked points to estimate the amount necessary that does not hit one of the\n  # percentages, take the multiple above it, the multiple below, average, and apply that to the \n  # FASTQ files, should be more accurate\n}\n\n",
    "created" : 1438794483095.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1964201950",
    "id" : "FC19BFA9",
    "lastKnownWriteTime" : 1438811010,
    "path" : "C:/Development/TCRSeqNormalization/tcr_seq_normalizer.R",
    "project_path" : "tcr_seq_normalizer.R",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "type" : "r_source"
}